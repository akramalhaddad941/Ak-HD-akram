# وحدة تمييز المتحدثين واللغات (Speaker & Language Diarization Unit)

تُعد وحدة تمييز المتحدثين واللغات مكونًا أساسيًا في نظام STT المتقدم، وهي مسؤولة عن تحليل المقاطع الصوتية لتحديد المتحدثين المختلفين وتحديد اللغة المنطوقة في كل مقطع. هذه المعلومات حيوية لإنتاج ترجمات دقيقة ومنسقة، خاصة في مقاطع الفيديو التي تحتوي على حوارات متعددة.

## الغرض والأهداف

*   **تمييز المتحدثين (Speaker Diarization)**: تحديد متى يتحدث كل شخص في التسجيل الصوتي، وتعيين هوية فريدة لكل متحدث (مثل: المتحدث 1، المتحدث 2).
*   **تحديد اللغة (Language Identification)**: التعرف التلقائي على اللغة المنطوقة في كل مقطع صوتي، وهو أمر بالغ الأهمية لاختيار محرك STT المناسب.
*   **تنسيق الترجمات**: استخدام معلومات المتحدثين لتنسيق الترجمات بشكل صحيح، مما يسهل على المشاهد تتبع الحوار.

## المكونات الرئيسية للوحدة

1.  **نموذج تحديد اللغة (Language Identification Model - LID)**:
    *   خوارزمية تعلم آلي تقوم بتحليل الخصائص الصوتية للمقطع الصوتي لتحديد اللغة المنطوقة (مثل الإنجليزية، العربية، الإسبانية).

2.  **نموذج تمييز المتحدثين (Speaker Diarization Model - SD)**:
    *   خوارزمية تقوم بتقسيم التسجيل الصوتي إلى مقاطع متجانسة بناءً على هوية المتحدث.
    *   تستخدم تقنيات مثل التجميع (Clustering) للخصائص الصوتية للمتحدثين.

3.  **وحدة دمج البيانات (Data Fusion Unit)**:
    *   تجمع المعلومات الناتجة عن نماذج LID و SD وتدمجها مع المقاطع الصوتية وطوابع الوقت.

## آلية العمل

1.  **استقبال المقاطع الصوتية**: تتلقى الوحدة المقاطع الصوتية المعالجة مسبقًا.
2.  **تحديد اللغة**: يتم تمرير كل مقطع صوتي إلى نموذج LID لتحديد اللغة المنطوقة.
3.  **تمييز المتحدثين**: يتم تمرير التسجيل الصوتي الكامل إلى نموذج SD لتحديد المتحدثين المختلفين وتعيين مقاطع الكلام لهم.
4.  **دمج البيانات**: يتم دمج معلومات اللغة والمتحدثين مع طوابع الوقت لكل مقطع.
5.  **إخراج البيانات**: يتم إخراج قائمة من المقاطع الصوتية، كل مقطع يحتوي على: طابع الوقت، اللغة، وهوية المتحدث.

## التنفيذ التقني (مثال)

```typescript
// src/interfaces/audio.ts (Extended)
export interface DiarizationResult {
  segmentId: string;
  startTime: number;
  endTime: number;
  speakerId: string; // e.g., 'Speaker_1', 'Speaker_2'
  languageCode: string; // e.g., 'ar-EG', 'en-US'
}

// src/services/speakerLanguageDiarizationUnit.ts
import { AudioSegment, DiarizationResult } from '../interfaces/audio';

export class SpeakerLanguageDiarizationUnit {
  async process(audioSegments: AudioSegment[]): Promise<DiarizationResult[]> {
    console.log(`Starting diarization and language identification for ${audioSegments.length} segments...`);
    const results: DiarizationResult[] = [];

    // Placeholder for actual Diarization and Language ID logic
    // In a real system, this would involve calling a specialized ML model (e.g., Pyannote, commercial APIs)

    let currentSpeaker = 1;
    for (const segment of audioSegments) {
      // 1. Simulate Language Identification
      const languageCode = this.identifyLanguage(segment.audioData);

      // 2. Simulate Speaker Diarization (simplified: alternate speaker for each segment)
      const speakerId = `Speaker_${currentSpeaker}`;
      currentSpeaker = currentSpeaker === 1 ? 2 : 1;

      results.push({
        segmentId: segment.id,
        startTime: segment.startTime,
        endTime: segment.endTime,
        speakerId: speakerId,
        languageCode: languageCode,
      });
    }

    console.log(`Finished diarization. Identified ${results.length} segments.`);
    return results;
  }

  private identifyLanguage(audioData: Buffer): string {
    // Placeholder: In a real scenario, analyze audio features to determine language
    // For demonstration, assume Arabic if a keyword is present, otherwise English
    const isArabic = audioData.toString('utf8').includes('عربي'); // Highly simplified
    return isArabic ? 'ar-EG' : 'en-US';
  }
}

export const speakerLanguageDiarizationUnit = new SpeakerLanguageDiarizationUnit();
```

تضمن هذه الوحدة أن يتم التعرف على السياق اللغوي والاجتماعي للحوار بدقة، مما يسهل عملية التحويل النصي والترجمة اللاحقة، ويحسن بشكل كبير من تجربة المستخدم النهائية.
